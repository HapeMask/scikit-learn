"""Variational von-Mises Fisher Mixture Model"""
import numpy as np
from scipy.optimize import fminbound

from ..externals.six.moves import xrange
from ..utils import as_float_array, array2d, check_random_state
from .gmm import GMM
from .dpgmm import log_normalize, digamma


def sample_sphere_3d(N=1, rng=None):
    rng = check_random_state(rng)
    X = rng.normal(size=(N, 3))
    return np.squeeze(X / np.sqrt((X**2.).sum(axis=1))[:, np.newaxis])


def axisangle2rotmat(axis, angle=None):
    if angle is None:
        angle = np.sqrt((axis**2.).sum())
        axis /= angle

    skew = np.array([
        [0., -axis[2], axis[1]],
        [axis[2], 0., -axis[0]],
        [-axis[1], axis[0], 0.]])
    skew2 = np.outer(axis, axis) - np.eye(3)

    return np.eye(3) + np.sin(angle)*skew + (1.-np.cos(angle))*skew2


def sample_vmf_3d(mean, prec, size=1, rng=None):
    """Generate samples from a given 3D vMF distribution.

    Samples are generated by sampling from the canonical (Z-axis mean) 3D vMF
    distribution and rotating the resulting points."""
    rng = check_random_state(rng)

    # As precision approaches 0, the distribution becomes a uniform
    # distribution on the unit sphere.
    if prec < 1e-8:
        return sample_sphere_3d(size, rng)

    # Sample from the canonical vMF distribution.
    xi = np.squeeze(rng.rand(size))[..., np.newaxis]
    phi = np.squeeze(2*np.pi*rng.rand(size))[..., np.newaxis]

    w = (1. + (1. / prec) * (np.log(xi) +
         np.log(1. - ((xi - 1.) / xi) * np.exp(-2.*prec))))

    n = np.sqrt(1. - w**2.)
    pts = np.hstack([n*np.cos(phi), n*np.sin(phi), w])

    mean = as_float_array(mean)
    mean /= np.sqrt((mean**2.).sum())

    # Compute the rotation matrix to rotate the Z axis onto the desired mean.
    if np.allclose(mean, [0., 0., 1.]):
        R = np.eye(3)
    else:
        angle = np.arccos(np.dot(mean, [0., 0., 1.]))
        if abs(angle - np.pi) < 1e-8:
            R = np.eye(3)
            R[2, 2] = -1.
        else:
            axis = np.cross([0., 0., 1.], mean)
            axis /= np.sqrt((axis**2.).sum())
            R = axisangle2rotmat(axis, angle)

    return np.dot(pts, R.T)


def log_c3k(k):
    """Log-normalization factor for the vMF distribution."""
    return np.log(k) - np.log(2.*np.pi) - np.log(np.exp(k) - np.exp(-k))


def coth(x):
    return np.cosh(x) / np.sinh(x)


def mean_rel_abs_diff(a, b):
    return (abs(a - b) / abs(a)).mean()


class VBMFMM(GMM):
    """Variational Inference for the 3D von-Mises Fisher Mixture Model

    Parameters
    ----------
    n_components : int, optional
        Number of mixture components. Defaults to 1.

    alpha : float, optional
        Real number representing the concentration parameter of the dirichlet
        distribution. Intuitively, the higher the value of alpha the more
        likely the model is to use as many components as possible. Defaults to
        1.

    mu_0 : array, shape (3, ), optional
        Mean parameter for the conjugate prior on component means. If None,
        mu_0 is set to the mean of X during fitting. Defaults to None.

    k_0 : float, optional
        Concentration parameter for the conjugate prior on component means.
        Defaults to 0.1.

    a : float, optional
        First parameter for the conjugate prior on component precisions.
        Defaults to 3.

    b : float, optional
        Second parameter for the conjugate prior on component precisions.
        Defaults to 2.

    params : string, optional
        Controls which parameters are updated in the training process. Can
        contain any combination of 'm' for means, and 'p' for precisions.
        Defaults to 'mp'.

    init_params : string, optional
        Controls which parameters are updated in the initialization process.
        Can contain any combination of 'm' for means, and 'p' for precisions.
        Defaults to 'mp'.

    n_iter : int, optional
        Maximum number of variational update iterations to perform.

    Attributes
    ----------
    `weights_` : array, shape (`n_components`, )
        Mixing weights for each mixture component.

    `means_` : array, shape (`n_components`, `n_features`)
        Mean parameters for each mixture component.

    `precs_` : array, shape (`n_components`, )
        Precision parameters for each mixture component.

    See Also
    --------
    GMM : Finite Gaussian mixture model fit with EM

    VBGMM : Finite gaussian mixture model fit with a variational
        algorithm, better for situations where there might be too little
        data to get a good estimate of the covariance matrix.

    DPGMM : Ininite gaussian mixture model, using the dirichlet
        process, fit with a variational algorithm
    """

    def __init__(self, n_components=10, alpha=1.0, mu_0=None, k_0=0.1, a=3.0,
                 b=2.0, n_iter=128, thresh=1e-6, verbose=False,
                 random_state=None, params='mp', init_params='mp'):

        if a <= b or a <= 0 or b <= 0:
            raise ValueError("Valid a and b must satisfy: a > b > 0.")

        self.alpha = alpha
        self.mu_0 = mu_0
        self.k_0 = k_0
        self.a = a
        self.b = b

        self.verbose = verbose

        super(VBMFMM, self).__init__(n_components,
                                     covariance_type='spherical',
                                     random_state=random_state, thresh=thresh,
                                     min_covar=None, n_iter=n_iter,
                                     params=params, init_params=init_params)

    def do_e_step_(self, X, N_bar):
        dg = (digamma(self.alpha + N_bar) -
              digamma(1 + N_bar.sum()))[np.newaxis]
        logprob = (log_c3k(self.precs_)[np.newaxis] +
                   np.dot(X, (self.precs_[:, np.newaxis] * self.means_).T))

        gamma = (dg + logprob)
        r = log_normalize(gamma, axis=-1)
        logprob = np.sum(r * logprob, axis=-1)
        return logprob, r

    def do_m_step_(self, X, r, k_0_mu_0):
        if 'm' in self.params:
            N_bar = r.sum(axis=0)
            x_nn = (X[:, np.newaxis] * r[:, :, np.newaxis]).sum(axis=0)
            x_bar = x_nn / N_bar[:, np.newaxis]

            self.means_ = ((N_bar * self.precs_)[:, np.newaxis] * x_bar +
                           k_0_mu_0)
            self.means_ /= np.sqrt((self.means_**2).sum(axis=1))[:, np.newaxis]

        if 'p' in self.params:
            c = -(self.b + (self.means_ * x_nn).sum(axis=1)) / (self.a + N_bar)

            for k in xrange(self.n_components):
                self.precs_[k] = fminbound(
                    lambda x: 0.5*((1./x) - coth(x) - c[k])**2,
                    1e-6, 200., maxfun=20, disp=0)

    def fit(self, X, y=None):
        X = array2d(X)
        if X.shape[1] != 3:
            raise ValueError('Can only fit 3-dimensional data.')

        N = X.shape[0]

        if self.mu_0 is None:
            mu_0 = X.mean(axis=0)
        else:
            mu_0 = self.mu_0
        mu_0 /= np.sqrt((mu_0**2).sum())

        k_0_mu_0 = self.k_0 * mu_0

        # Initialize parameters and state.
        if 'm' in self.init_params or not hasattr(self, 'means_'):
            self.means_ = sample_sphere_3d(self.n_components,
                                           self.random_state)
        if 'p' in self.init_params or not hasattr(self, 'precs_'):
            self.precs_ = 1e-2 * np.ones((self.n_components, ))

        r = np.ones((N, self.n_components)) / float(self.n_components)

        self.converged_ = False
        for it in xrange(self.n_iter):
            prev_means = self.means_.copy()
            prev_precs = self.precs_.copy()

            self.N_bar_ = r.sum(axis=0)
            logprob, r = self.do_e_step_(X, self.N_bar_)
            self.do_m_step_(X, r, k_0_mu_0)

            if self.verbose:
                print('Logprob at iteration %d: %1.5e' % (it, logprob.sum()))

            mrad_mu = mean_rel_abs_diff(self.means_, prev_means)
            mrad_k = mean_rel_abs_diff(self.precs_, prev_precs)

            if mrad_mu < self.thresh and mrad_k < self.thresh:
                if self.verbose:
                    print('Converged at iteration', it)
                self.converged_ = True
                break

        self.weights_ = ((self.alpha + self.N_bar_) /
                         (self.n_components * self.alpha + N))

        return self

    def fit_predict(self, X, y=None):
        return self.fit(X, y).predict(X)

    def score_samples(self, X):
        return self.do_e_step_(X, self.N_bar_)
